# LoRA

Lora微调实际上是指一种特定的模型微调技术，称为"LoRA"，全称为"Low-Rank Adaptation"（低秩适配）。核心思想是在模型的预训练权重基础上，
通过引入额外的、较小的、可训练的参数矩阵来实现微调，这些矩阵作为原始权重的低秩更新。


## 原理


## 流程


## 实战